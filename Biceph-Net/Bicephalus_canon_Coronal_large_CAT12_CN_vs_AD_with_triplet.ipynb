{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL**"
      ],
      "metadata": {
        "id": "WuacCR2GIgJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8vmV21P_2pR"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Bidirectional, Lambda, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.training import moving_averages\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
        "                 n_classes=10, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        #self.class_path = class_path\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            tmp = np.load('/media/iitindmaths/Seagate_Expansion_Drive/Bup_Backup/SPM/alzheimers-disease/CN_vs_AD/npy_large/smwp1/' + ID )\n",
        "            #tmp_rgb = gray2rgb(tmp)\n",
        "            X[i,] = tmp\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "        return X, y\n",
        "\n",
        "#returns a list  of  hyperparamter settings\n",
        "def load_hyperparameter_settings(sampler_file):\n",
        "    hyp_list = []\n",
        "    with open(sampler_file, \"rb\") as obj:\n",
        "        for i in range(10):\n",
        "            hyp_list.append(pickle.load(obj))\n",
        "    return hyp_list\n",
        "\n",
        "#returns resNet base_model\n",
        "def load_base_model_vgg16(img_width,img_height,num_channels,weight_init='imagenet',include_fc_layers=False):\n",
        "    base_model = applications.VGG16(include_top=include_fc_layers,weights=weight_init, input_shape=(img_width, img_height, num_channels), input_tensor=None, pooling=None)\n",
        "    return base_model\n",
        "\n",
        "# set the first num_layers to nontrainable\n",
        "# model - an instance of Keras Model\n",
        "# => model is the final model (base_model added with fully connected layers)\n",
        "\n",
        "def set_nontrainable_layers(num_layers, model):\n",
        "    for layer in model.layers[:num_layers]:\n",
        "        layer.trainable = False\n",
        "    return model\n",
        "\n",
        "#returns the dict of cross validation settings\n",
        "\n",
        "def load_cross_validation_settings(cv_file):\n",
        "    cv_setting = None\n",
        "    with open(cv_file, \"rb\") as obj:\n",
        "        cv_setting = pickle.load(obj)\n",
        "    return cv_setting\n",
        "    \n",
        "\n",
        "def save_model_history(history, history_path):    \n",
        "    df_train_loss = pd.DataFrame(history.history['loss'])\n",
        "    df_train_loss.columns = ['train_loss']\n",
        "    df_val_loss = pd.DataFrame(history.history['val_loss'])\n",
        "    df_val_loss.columns = ['validation_loss']\n",
        "    df_history = pd.concat([df_train_loss,df_val_loss], axis=1)\n",
        "    df_history.to_csv(history_path+\"/VGG_CLF_AD_1365_24_05_21.csv\", index=False)\n",
        "    return    \n",
        "    \n",
        "def fit_generator(model, training_generator, validation_generator, checkpoint_path):    \n",
        "    cb_save_path = ''\n",
        "    lrate_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=5,verbose=1,min_lr=0.0000001)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint('/media/iitindmaths/Seagate_Expansion_Drive/Bup_Backup/SPM/alzheimers-disease/CN_vs_AD/model_checkpoints/VGG_CLF_AD_1365_24_05_21_{epoch:08d}.h5', \n",
        "                                     save_weights_only=True, period=1)\n",
        "    history = model.fit(\n",
        "                training_generator,\n",
        "                epochs = 100,\n",
        "                verbose = 1,\n",
        "                validation_data = validation_generator,\n",
        "                callbacks = [lrate_reduce,mc]\n",
        "            )\n",
        "    return history \n",
        "\n",
        "def fit_crossvalidation(model, cv_file, generator_params_dict, datapath, checkpoint_path, history_path):\n",
        "    cv_setting = load_cross_validation_settings(cv_file)   \n",
        "    full_train_dict = cv_setting['train']\n",
        "    full_val_dict = cv_setting['validation']\n",
        "    \n",
        "    train_subject_id = full_train_dict['subject_dict']\n",
        "    train_subject_group = full_train_dict['subject_group']\n",
        "    train_subject_slices = full_train_dict['subject_slices']\n",
        "    train_class_ID = []\n",
        "    train_class_label = {}\n",
        "    train_subj_slices = []\n",
        "        \n",
        "    for idx in train_subject_id.keys():\n",
        "        tmp_id = train_subject_id[idx]\n",
        "        tmp_grp = train_subject_group[idx]\n",
        "        tmp_slices = train_subject_slices[idx]\n",
        "        label = None\n",
        "        if tmp_grp ==  'CN':\n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1\n",
        "        for i in tmp_slices:\n",
        "            train_subj_slices.append(i)\n",
        "            train_class_label[i] = label\n",
        "    \n",
        "    val_subject_id = full_val_dict['subject_dict']\n",
        "    val_subject_group = full_val_dict['subject_group']\n",
        "    val_subject_slices = full_val_dict['subject_slices']\n",
        "    val_class_ID = []\n",
        "    val_class_label = {}\n",
        "    val_subj_slices = []\n",
        "    \n",
        "    for idx in val_subject_id.keys():\n",
        "        tmp_id = val_subject_id[idx]\n",
        "        tmp_grp = val_subject_group[idx]\n",
        "        tmp_slices = val_subject_slices[idx]\n",
        "        label = None\n",
        "        if tmp_grp ==  'CN':\n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1\n",
        "        for i in tmp_slices:\n",
        "            val_subj_slices.append(i)\n",
        "            val_class_label[i] = label\n",
        "        \n",
        "    training_generator = DataGenerator(train_subj_slices,train_class_label,**generator_params_dict)\n",
        "        \n",
        "    val_params_dict = generator_params_dict.copy()\n",
        "    val_params_dict['shuffle'] = False\n",
        "    \n",
        "    validation_generator = DataGenerator(val_subj_slices,val_class_label,**val_params_dict)\n",
        "    history = fit_generator(model, training_generator, validation_generator, checkpoint_path)\n",
        "    save_model_history(history,history_path)\n",
        "    return\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zHESH9u_2pW"
      },
      "outputs": [],
      "source": [
        "img_width = 121\n",
        "img_height = 121\n",
        "channels = 3\n",
        "\n",
        "datapath = '/media/iitindmaths/Seagate_Expansion_Drive/Bup_Backup/SPM/alzheimers-disease/CN_vs_AD/npy_large/smwp1_coronal'\n",
        "params_dict = {'dim':(img_width,img_height),\n",
        "                   'n_channels': 3,\n",
        "                   'P': 20,\n",
        "                   'K': 4,\n",
        "                   #'shuffle':True\n",
        "                  }\n",
        "history_path = '/media/iitindmaths/Seagate_Expansion_Drive/Bup_Backup/SPM/alzheimers-disease/CN_vs_AD/history'\n",
        "checkpoint_path = '/media/iitindmaths/Seagate_Expansion_Drive/Bup_Backup/SPM/alzheimers-disease/CN_vs_AD/model_checkpoints'\n",
        "cv_file = '/media/iitindmaths/Seagate_Expansion_Drive/Alz_CN_vs_AD_1365_each_full_subject_list_coronal.pkl'\n",
        "    \n",
        "#clf - bicephalus\n",
        "vgg = tf.keras.applications.VGG16(include_top=False,input_shape=(img_width, img_height, channels))\n",
        "vgg_l1 = Conv2D(256, (1,1), padding='same')(vgg.output)\n",
        "vgg_l2 = Conv2D(128, (1,1), padding='same')(vgg_l1)\n",
        "vgg_l3 = Conv2D(64, (1,1), padding='same')(vgg_l2)\n",
        "vgg_flat = Flatten()(vgg_l3)\n",
        "vgg_dense = Dense(64, activation=None)(vgg_flat)\n",
        "vgg_lambda = Lambda(lambda x: tf.math.l2_normalize(x, axis=1),name='triplet_nw')(vgg_dense)\n",
        "\n",
        "fc1 = Dense(64, activation='relu')(vgg_flat)\n",
        "vgg_cre = Dense(1, activation='sigmoid', name='binary_cre')(fc1)\n",
        "\n",
        "vgg_concat = tf.keras.layers.Concatenate()([vgg_lambda, vgg_cre])\n",
        "\n",
        "final_dense1 = Dense(32, activation='relu')(vgg_concat)\n",
        "final_dense2 = Dense(1, activation='sigmoid',name='binary_final')(final_dense1)\n",
        "\n",
        "losses = {'triplet_nw':tfa.losses.TripletSemiHardLoss(),\n",
        "         'binary_final':tf.keras.losses.BinaryCrossentropy(from_logits=False)}\n",
        "\n",
        "lossWeights = {\"triplet_nw\": 1.0, \"binary_final\":1.0}\n",
        "metrics = {\"triplet_nw\":tfa.losses.TripletSemiHardLoss() , \"binary_final\":\"accuracy\"}\n",
        "\n",
        "model = Model(inputs = vgg.input, outputs = [vgg_lambda,final_dense2])\n",
        "model.load_weights('/media/iitindmaths/Seagate_Expansion_Drive/Bup_Backup/SPM/alzheimers-disease/CN_vs_AD/model_checkpoints/Bicephalus_AD_1365_margin_pat2_1_12_06_21_coronal_00000015.h5')\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "    loss=losses,loss_weights=lossWeights,metrics=metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnA24TOr_2pd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Bicephalus-canon-Coronal-large-CAT12-CN-vs-AD-with-triplet.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}